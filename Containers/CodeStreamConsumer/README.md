# Can we process the entire data? What causes it to hang? How can we modify it to work?
No, we are not able to process the entire Qualitas Corpus. It crashes randomly when reading the next file, somewhere after reading 10 000 files to 120 000 files the crash happens. Another issue that we have run into is that the CodeStreamConsumer stops discovering clones after saving around 3000 clones. We believe this is due to a promise not returning after that point. To resolve these issues we recommend modifying the application to store the clones in a database to free up the memory and hopefully make the application run better and keep promises.

# What can be done to reduce the number of comparisons?
One common solution that allows clone detectors to do fewer comparisons is to hash the code chunks to be compared. If we hash 5-line chunks and compare the hashes instead, we’ve reduced the amount of comparisons by 5 times. This could create challenges when expanding clone segments and also runs the risk of hash collisions. The latter can be mitigated by using a good hash function. Hashing also requires us to store the hashed version, requiring more storage, and more processing is needed to hash every new file encountered. However, the comparisons will perform better, so we might see a slower increase in time taken per file after we’ve analyzed many files.

# Timing trends
A clear trend that appears when running the application over longer periods of time is that it takes longer for the application to find a match or even check if the file has a match. This is expected as the amount of clones increases for each process file and the application should compare between more possible clone candidates as each previous file is a possible clone candidate. Since we need to check every other file per new file, the worst case performance is O(n), which grows linearly.

# Briefly about our implementation
For our filterCloneCandidates, we take each chunk in the file and compare that single chunk to every chunk in the compare file. If a chunk were to match then we have a match and create a new Clone that will be used later in the expandCloneCandidates. The expandCloneCandidates function works by trying maybeExpandWith to expand the current clone with another one. If it successfully expands, it tries expanding forward with the next clone. If it doesn’t expand, the current clone must be maximally expanded and is pushed to the list of unique clones. We noticed the expand clones code is not perfect since it reports 4 clones in the A and B test files while we only expect 3 clones. We were not able to improve it further without somehow impacting the consolidate function. Speaking of which, the consolidate function works by checking every detected clone in the current file against every other discovered clone in all other files. If it finds a match, it adds itself to the list of targets of that match. If it doesn’t match another clone, it is treated as unique. We added a /timers endpoint to the webserver, which shows average times for the last 10, 100, and 1000 files processed, as well as averages of 5000-file intervals (0-5000, 5000-10000 etc.). We also added a scatter plot showing the times of every processed file, where the x-axis of the plot represents the file number and the y-axis represents the total processing time for that file in microseconds.

# Future improvements
We would like to improve the expand function as we know it isn’t perfect. Following the tips and assumptions, this is the best we could come up with.
We also suggest that a database is implemented to store the clones in so that one entry exists in the webpage per clone found. A problem now is that clones are stored in an object called CloneStorage and we have little to no control over the content within that collection of objects.
If we do not want a database-based solution to this problem, then another solution for this is to go through the CloneStorage collection and remove duplicates as we now have an entry for every time a clone is found. Lastly, we believe the promise chain is a weak point in the process because it stops returning at some point without reporting any failures or timeouts.
